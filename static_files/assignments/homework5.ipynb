{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYPuQ5VRtVXp"
      },
      "source": [
        "# DS 542 - 2025 Fall - Homework 5\n",
        "\n",
        "In this homework, you will practice using PyTorch's frameworks for repeatable data management and reusable model designs.\n",
        "You will also track gradient statistics during the fitting process.\n",
        "\n",
        "When you are done writing code, make sure to run all the cells and then submit your notebook in [Gradescope](https://www.gradescope.com/courses/1071076).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSF0GvD0TgV"
      },
      "source": [
        "## Problem 1 - Setup Dataset and DataLoader Objects\n",
        "\n",
        "PyTorch provides various utilities to help managing large data sets.\n",
        "In this problem, you will implement `Dataset` and `DataLoader` objects for the Pima Indians Diabetes data set.\n",
        "This data set is small and easily fits in memory, but these objects will also help with randomization and batching for stochastic gradient descent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv-QdpAr20o4"
      },
      "source": [
        "Here is a link to PyTorch's [Datasets & DataLoaders tutorial](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGR8SPRjyWd0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30JCw4JtyP4T"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://github.com/npradaschnor/Pima-Indians-Diabetes-Dataset/raw/refs/heads/master/diabetes.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv_wHdCd1mci"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SjxA1G91uA3"
      },
      "source": [
        "Finish the implementation of the `DiabetesDataset` class below by implementing the missing methods for `torch.utils.data.Dataset`.\n",
        "The dataset should return pairs of tensors where the first tensor is the input row and the second tensor has the corresponding `Outcome` target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYy3PRRn1M-O"
      },
      "outputs": [],
      "source": [
        "class DiabetesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe.drop('Outcome', axis=1).values\n",
        "        self.targets = dataframe['Outcome'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAsEy7MS1UNK"
      },
      "source": [
        "Create a DataLoader object using an instance of your `DiabetesDataSet` class and configure it to randomize the data and return batches of 100 rows at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI98yVUU1SvG"
      },
      "outputs": [],
      "source": [
        "dataset = DiabetesDataset(df)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1MVuFm_4RYD"
      },
      "source": [
        "Test your data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsSrmBVk4Ang"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "for batch_input, batch_output in dataloader:\n",
        "    print(\"INPUT\", batch_input)\n",
        "    print(\"OUTPUT\", batch_output)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQN5oa87uUcM"
      },
      "source": [
        "## Problem 2 - Use Adam to Optimize Logistic Regression\n",
        "\n",
        "Write a training loop using PyTorch's [`torch.optim.Adam`](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html) to optimize logistic regression.\n",
        "Use the following `LogisticRegression` class for the implementation of logistic regression and [`torch.nn.functional.binary_cross_entropy`](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html) for the loss function.\n",
        "\n",
        "Run the training loop for 10 epochs printing the average training batch loss for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsWgbTdryuZn"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # use torch.nn.Parameter to register these as model parameters\n",
        "        self.weights = torch.nn.Parameter(torch.zeros(len(df.columns)-1, 1))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(x @ self.weights + self.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fybVGPI6Cul5"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9u8dbehE6TS"
      },
      "source": [
        "## Problem 3 - Track Training Statistics and Gradients\n",
        "\n",
        "Copy your training loop from problem 2 and modify it as follows.\n",
        "\n",
        "1. Increase the number of epochs to 100.\n",
        "2. Track the training loss of each batch.\n",
        "3. Track the training accuracy of each batch.\n",
        "4. Track the loss gradient of each batch for both the weights and bias of the logistic regression.\n",
        "5. After the training loop is done, plot the data from 2-4. Use Matplotlib's subplot function to stack the charts vertically so they are aligned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVPeZnG-GSTr"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model and optimizer\n",
        "model = LogisticRegression()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = torch.nn.functional.binary_cross_entropy\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "# Lists to store training statistics and gradients\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "weight_gradients = []\n",
        "bias_gradients = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    for batch_input, batch_output in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_input)\n",
        "        loss = criterion(outputs, batch_output)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track loss\n",
        "        train_losses.append(loss.item())\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Track accuracy\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct_predictions += (predicted == batch_output).sum().item()\n",
        "        total_samples += batch_output.size(0)\n",
        "        train_accuracies.append(correct_predictions / total_samples)\n",
        "\n",
        "        # Track gradients\n",
        "        weight_gradients.append(model.weights.grad.norm().item())\n",
        "        bias_gradients.append(model.bias.grad.norm().item())\n",
        "\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB38bpH5GdPV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "# plot training loss\n",
        "plt.plot(train_losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "# plot training accuracy\n",
        "plt.plot(train_accuracies)\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "# plot weights gradient\n",
        "plt.plot(bias_gradients, label=\"bias\")\n",
        "for i in range(len(model.weights.grad)):\n",
        "    plt.plot([w[i] for w in weight_gradients], label=f\"{df.columns[i]}\") # Assuming order matches columns\n",
        "plt.title(\"Gradients\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Gradient\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplots_adjust(hspace=1.0)\n",
        "plt.show();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}